.. _svm:

=======================
Support Vector Machines
=======================

.. currentmodule:: sklearn.svm

**Support vector machines (SVMs)** are a set of supervised learning
methods used for :ref:`classification <svm_classification>`,
:ref:`regression <svm_regression>` and :ref:`outliers detection
<svm_outlier_detection>`.

The advantages of support vector machines are:

    - Effective in high dimensional spaces.

    - Still effective in cases where number of dimensions is greater
      than the number of samples.

    - Uses a subset of training points in the decision function (called
      support vectors), so it is also memory efficient.

    - Versatile: different :ref:`svm_kernels` can be
      specified for the decision function. Common kernels are
      provided, but it is also possible to specify custom kernels.

The disadvantages of support vector machines include:

    - If the number of features is much greater than the number of
      samples, avoid over-fitting in choosing :ref:`svm_kernels` and regularization
      term is crucial.

    - SVMs do not directly provide probability estimates, these are
      calculated using an expensive five-fold cross-validation
      (see :ref:`Scores and probabilities <scores_probabilities>`, below).

The support vector machines in scikit-learn support both dense
(``numpy.ndarray`` and convertible to that by ``numpy.asarray``) and
sparse (any ``scipy.sparse``) sample vectors as input. However, to use
an SVM to make predictions for sparse data, it must have been fit on such
data. For optimal performance, use C-ordered ``numpy.ndarray`` (dense) or
``scipy.sparse.csr_matrix`` (sparse) with ``dtype=float64``.


.. _svm_classification:

Classification
==============

:class:`SVC`, :class:`NuSVC` and :class:`LinearSVC` are classes
capable of performing multi-class classification on a dataset.


.. figure:: ../auto_examples/svm/images/sphx_glr_plot_iris_001.png
   :target: ../auto_examples/svm/plot_iris.html
   :align: center


:class:`SVC` and :class:`NuSVC` are similar methods, but accept
slightly different sets of parameters and have different mathematical
formulations (see section :ref:`svm_mathematical_formulation`). On the
other hand, :class:`LinearSVC` is another implementation of Support
Vector Classification for the case of a linear kernel. Note that
:class:`LinearSVC` does not accept keyword ``kernel``, as this is
assumed to be linear. It also lacks some of the members of
:class:`SVC` and :class:`NuSVC`, like ``support_``.

As other classifiers, :class:`SVC`, :class:`NuSVC` and
:class:`LinearSVC` take as input two arrays: an array X of size ``[n_samples,
n_features]`` holding the training samples, and an array y of class labels
(strings or integers), size ``[n_samples]``::


    >>> from sklearn import svm
    >>> X = [[0, 0], [1, 1]]
    >>> y = [0, 1]
    >>> clf = svm.SVC()
    >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)

After being fitted, the model can then be used to predict new values::

    >>> clf.predict([[2., 2.]])
    array([1])

SVMs decision function depends on some subset of the training data,
called the support vectors. Some properties of these support vectors
can be found in members ``support_vectors_``, ``support_`` and
``n_support``::

    >>> # get support vectors
    >>> clf.support_vectors_
    array([[ 0.,  0.],
           [ 1.,  1.]])
    >>> # get indices of support vectors
    >>> clf.support_ # doctest: +ELLIPSIS
    array([0, 1]...)
    >>> # get number of support vectors for each class
    >>> clf.n_support_ # doctest: +ELLIPSIS
    array([1, 1]...)

.. _svm_multi_class:

Multi-class classification
--------------------------

:class:`SVC` and :class:`NuSVC` implement the "one-against-one"
approach (Knerr et al., 1990) for multi- class classification. If
``n_class`` is the number of classes, then ``n_class * (n_class - 1) / 2``
classifiers are constructed and each one trains data from two classes.
To provide a consistent interface with other classifiers, the
``decision_function_shape`` option allows to aggregate the results of the
"one-against-one" classifiers to a decision function of shape ``(n_samples,
n_classes)``::

    >>> X = [[0], [1], [2], [3]]
    >>> Y = [0, 1, 2, 3]
    >>> clf = svm.SVC(decision_function_shape='ovo')
    >>> clf.fit(X, Y) # doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
    >>> dec = clf.decision_function([[1]])
    >>> dec.shape[1] # 4 classes: 4*3/2 = 6
    6
    >>> clf.decision_function_shape = "ovr"
    >>> dec = clf.decision_function([[1]])
    >>> dec.shape[1] # 4 classes
    4

On the other hand, :class:`LinearSVC` implements "one-vs-the-rest"
multi-class strategy, thus training n_class models. If there are only
two classes, only one model is trained::

    >>> lin_clf = svm.LinearSVC()
    >>> lin_clf.fit(X, Y) # doctest: +NORMALIZE_WHITESPACE
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
         verbose=0)
    >>> dec = lin_clf.decision_function([[1]])
    >>> dec.shape[1]
    4

See :ref:`svm_mathematical_formulation` for a complete description of
the decision function.

Note that the :class:`LinearSVC` also implements an alternative multi-class
strategy, the so-called multi-class SVM formulated by Crammer and Singer, by
using the option ``multi_class='crammer_singer'``. This method is consistent,
which is not true for one-vs-rest classification.
In practice, one-vs-rest classification is usually preferred, since the results
are mostly similar, but the runtime is significantly less.

For "one-vs-rest" :class:`LinearSVC` the attributes ``coef_`` and ``intercept_``
have the shape ``[n_class, n_features]`` and ``[n_class]`` respectively.
Each row of the coefficients corresponds to one of the ``n_class`` many
"one-vs-rest" classifiers and similar for the intercepts, in the
order of the "one" class.

In the case of "one-vs-one" :class:`SVC`, the layout of the attributes
is a little more involved. In the case of having a linear kernel,
The layout of ``coef_`` and ``intercept_`` is similar to the one
described for :class:`LinearSVC` described above, except that the shape of
``coef_`` is ``[n_class * (n_class - 1) / 2, n_features]``, corresponding to as
many binary classifiers. The order for classes
0 to n is "0 vs 1", "0 vs 2" , ... "0 vs n", "1 vs 2", "1 vs 3", "1 vs n", . .
. "n-1 vs n".

The shape of ``dual_coef_`` is ``[n_class-1, n_SV]`` with
a somewhat hard to grasp layout.
The columns correspond to the support vectors involved in any
of the ``n_class * (n_class - 1) / 2`` "one-vs-one" classifiers.
Each of the support vectors is used in ``n_class - 1`` classifiers.
The ``n_class - 1`` entries in each row correspond to the dual coefficients
for these classifiers.

This might be made more clear by an example:

Consider a three class problem with class 0 having three support vectors
:math:`v^{0}_0, v^{1}_0, v^{2}_0` and class 1 and 2 having two support vectors
:math:`v^{0}_1, v^{1}_1` and :math:`v^{0}_2, v^{1}_2` respectively.  For each
support vector :math:`v^{j}_i`, there are two dual coefficients.  Let's call
the coefficient of support vector :math:`v^{j}_i` in the classifier between
classes :math:`i` and :math:`k` :math:`\alpha^{j}_{i,k}`.
Then ``dual_coef_`` looks like this:

+------------------------+------------------------+------------------+
|:math:`\alpha^{0}_{0,1}`|:math:`\alpha^{0}_{0,2}`|Coefficients      |
+------------------------+------------------------+for SVs of class 0|
|:math:`\alpha^{1}_{0,1}`|:math:`\alpha^{1}_{0,2}`|                  |
+------------------------+------------------------+                  |
|:math:`\alpha^{2}_{0,1}`|:math:`\alpha^{2}_{0,2}`|                  |
+------------------------+------------------------+------------------+
|:math:`\alpha^{0}_{1,0}`|:math:`\alpha^{0}_{1,2}`|Coefficients      |
+------------------------+------------------------+for SVs of class 1|
|:math:`\alpha^{1}_{1,0}`|:math:`\alpha^{1}_{1,2}`|                  |
+------------------------+------------------------+------------------+
|:math:`\alpha^{0}_{2,0}`|:math:`\alpha^{0}_{2,1}`|Coefficients      |
+------------------------+------------------------+for SVs of class 2|
|:math:`\alpha^{1}_{2,0}`|:math:`\alpha^{1}_{2,1}`|                  |
+------------------------+------------------------+------------------+


.. _scores_probabilities:

Scores and probabilities
------------------------

The :class:`SVC` method ``decision_function`` gives per-class scores 
for each sample (or a single score per sample in the binary case).
When the constructor option ``probability`` is set to ``True``,
class membership probability estimates
(from the methods ``predict_proba`` and ``predict_log_proba``) are enabled.
In the binary case, the probabilities are calibrated using Platt scaling:
logistic regression on the SVM's scores,
fit by an additional cross-validation on the training data.
In the multiclass case, this is extended as per Wu et al. (2004).

Needless to say, the cross-validation involved in Platt scaling
is an expensive operation for large datasets.
In addition, the probability estimates may be inconsistent with the scores,
in the sense that the "argmax" of the scores
may not be the argmax of the probabilities.
(E.g., in binary classification,
a sample may be labeled by ``predict`` as belonging to a class
that has probability <Â½ according to ``predict_proba``.)
Platt's method is also known to have theoretical issues.
If confidence scores are required, but these do not have to be probabilities,
then it is advisable to set ``probability=False``
and use ``decision_function`` instead of ``predict_proba``.

.. topic:: References:

 * Wu, Lin and Weng,
   `"Probability estimates for multi-class classification by pairwise coupling"
   <http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf>`_,
   JMLR 5:975-1005, 2004.
 
 
 * Platt
   `"Probabilistic outputs for SVMs and comparisons to regularized likelihood methods"
   <http://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf>`.

Unbalanced problems
--------------------

In problems where it is desired to give more importance to certain
classes or certain individual samples keywords ``class_weight`` and
``sample_weight`` can be used.

:class:`SVC` (but not :class:`NuSVC`) implement a keyword
``class_weight`` in the ``fit`` method. It's a dictionary of the form
``{class_label : value}``, where value is a floating point number > 0
that sets the parameter ``C`` of class ``class_label`` to ``C * value``.

.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png
   :target: ../auto_examples/svm/plot_separating_hyperplane_unbalanced.html
   :align: center
   :scale: 75


:class:`SVC`, :class:`NuSVC`, :class:`SVR`, :class:`NuSVR` and
:class:`OneClassSVM` implement also weights for individual samples in method
``fit`` through keyword ``sample_weight``. Similar to ``class_weight``, these
set the parameter ``C`` for the i-th example to ``C * sample_weight[i]``.


.. figure:: ../auto_examples/svm/images/sphx_glr_plot_weighted_samples_001.png
   :target: ../auto_examples/svm/plot_weighted_samples.html
   :align: center
   :scale: 75


.. topic:: Examples:

 * :ref:`sphx_glr_auto_examples_svm_plot_iris.py`,
 * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,
 * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`
 * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,
 * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`
 * :ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`,


.. _svm_regression:

Regression
==========

The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.

The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function for building the model ignores any training
data close to the model prediction.

There are three different implementations of Support Vector Regression: 
:class:`SVR`, :class:`NuSVR` and :class:`LinearSVR`. :class:`LinearSVR` 
provides a faster implementation than :class:`SVR` but only considers
linear kernels, while :class:`NuSVR` implements a slightly different
formulation than :class:`SVR` and :class:`LinearSVR`. See
:ref:`svm_implementation_details` for further details.

As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values::

    >>> from sklearn import svm
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> clf = svm.SVR()
    >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
        kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
    >>> clf.predict([[1, 1]])
    array([ 1.5])


.. topic:: Examples:

 * :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`



.. _svm_outlier_detection:

Density estimation, novelty detection
=======================================

One-class SVM is used for novelty detection, that is, given a set of
samples, it will detect the soft boundary of that set so as to
classify new points as belonging to that set or not. The class that
implements this is called :class:`OneClassSVM`.

In this case, as it is a type of unsupervised learning, the fit method
will only take as input an array X, as there are no class labels.

See, section :ref:`outlier_detection` for more details on this usage.

.. figure:: ../auto_examples/svm/images/sphx_glr_plot_oneclass_001.png
   :target: ../auto_examples/svm/plot_oneclass.html
   :align: center
   :scale: 75


.. topic:: Examples:

 * :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py`
 * :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`


å¤æåº¦
==========

æ¯æåéæºæ¯ä¸ªå¼ºå¤§çå·¥å·ï¼ä¸è¿å®çè®¡ç®åå­å¨ç©ºé´è¦æ±ä¹ä¼éçè¦è®­ç»åéçæ°ç®å¢å èå¿«éå¢å ã
SVMçæ ¸å¿æ¯ä¸ä¸ªäºæ¬¡è§åé®é¢(Quadratic Programming, QP)ï¼æ¯å°æ¯æåéåè®­ç»æ°æ®çå¶ä½é¨ååç¦»å¼æ¥ã
å¨å®è·µä¸­(æ°æ®éç¸å³)ï¼ä¼æ ¹æ® `libsvm`_ çç¼å­æå¤æï¼å¨ :math:`O(n_{features} \times n_{samples}^2)` å 
:math:`O(n_{features} \times n_{samples}^3)` ä¹é´åºäº `libsvm`_ çç¼©æ¾æä½æä¼è°ç¨è¿ä¸ª QPè§£æå¨ã
å¦ææ°æ®æ¯éå¸¸ç¨çï¼é£ :math:`n_{features}`  å°±ç¨æ ·æ¬åéä¸­éé¶ç¹å¾çå¹³åæ°éå»æ¿æ¢ã 

å¦å¤è¯·æ³¨æï¼å¨çº¿æ§æåµä¸ï¼ç± `liblinear`_ æä½ç :class:`LinearSVC` ç®æ³è¦æ¯ç±å®ç `libsvm`_ å¯¹åºç 
:class:`SVC` æ´ä¸ºé«æï¼å¹¶ä¸å®å ä¹å¯ä»¥çº¿æ§ç¼©æ¾å°æ°ç¾ä¸æ ·æ¬æèç¹å¾ã


ä½¿ç¨çªé¨
=====================


  * **é¿åæ°æ®å¤å¶**: å¯¹äº :class:`SVC`ï¼ :class:`SVR`ï¼ :class:`NuSVC` å
    :class:`NuSVR`ï¼ å¦ææ°æ®æ¯éè¿æäºæ¹æ³èä¸æ¯ç¨Cæåºçè¿ç»­åç²¾åº¦ï¼é£å®åä¼è°ç¨åºå±çCå½ä»¤åå¤å¶ã
    æ¨å¯ä»¥éè¿æ£æ¥å®ç ``flags`` å±æ§ï¼æ¥ç¡®å®ç»å®çnumpyæ°ç»æ¯ä¸æ¯Cè¿ç»­çã

    å¯¹äº :class:`LinearSVC` (å :class:`LogisticRegression
    <sklearn.linear_model.LogisticRegression>`) çä»»ä½è¾å¥ï¼é½ä¼ä»¥numpyæ°ç»å½¢å¼ï¼è¢«å¤å¶åè½¬æ¢ä¸º
    ç¨liblinearåé¨ç¨çæ°æ®å»è¡¨è¾¾ï¼åç²¾åº¦æµ®ç¹åfloatåéé¶é¨åçint32ç´¢å¼ï¼ã 
    å¦ææ¨æ³è¦ä¸ä¸ªéåå¤§è§æ¨¡ççº¿æ§åç±»å¨ï¼åä¸æç®å¤å¶ä¸ä¸ªå¯éçC-contiguousåç²¾åº¦numpyæ°ç»ä½ä¸ºè¾å¥ï¼
    é£æä»¬å»ºè®®æ¨å»ä½¿ç¨ :class:`SGDClassifier
    <sklearn.linear_model.SGDClassifier>` ç±»ä½ä¸ºæ¿ä»£ãç®æ å½æ°å¯ä»¥éç½®ä¸ºå :class:`LinearSVC`
    æ¨¡åå·®ä¸å¤ç¸åçã

  * **åæ ¸çç¼å­å¤§å°**: å¨å¤§è§æ¨¡é®é¢ä¸ï¼å¯¹äº :class:`SVC`, :class:`SVR`, :class:`nuSVC` å
    :class:`NuSVR`, åæ ¸ç¼å­çå¤§å°ä¼ç¹å«å½±åå°è¿è¡æ¶é´ãå¦ææ¨æè¶³å¤å¯ç¨çRAMï¼ä¸å¦¨æå®ç ``ç¼å­å¤§å°`` 
    è®¾å¾æ¯é»è®¤ç200(MB)è¦é«ï¼ä¾å¦ä¸º 500(MB) æè 1000(MB)ã

  * **æ©ç½ç³»æ°Cçè®¾ç½®**:å¨åççæåµä¸ï¼ ``C`` çé»è®¤éæ©ä¸º ``1`` ãå¦ææ¨æå¾å¤æ··æçè§å¯æ°æ®ï¼
    æ¨åºè¯¥è¦å»è°å°å®ã ``C`` è¶å°ï¼å°±è½æ´å¥½å°å»æ­£è§åä¼°è®¡ã

  * æ¯æåéæºç®æ³æ¬èº«ä¸æ¯ç¨æ¥æ©å¤§ä¸åæ§ï¼æä»¥ **æä»¬å¼ºçå»ºè®®æ¨å»æ©å¤§æ°æ®é**. ä¸¾ä¸ªä¾å­ï¼å¯¹äºè¾å¥åéXï¼
    è§æ´å®çæ¯ä¸ªæ°å¼èå´ä¸º[0, 1]æ[-1, +1]ï¼æèæ ååå®çä¸ºåå¼ä¸º0æ¹å·®ä¸º1çæ°æ®åå¸ãè¯·æ³¨æï¼
    ç¸åçç¼©æ¾æ åå¿é¡»è¦åºç¨å°ææçæµè¯åéï¼ä»èè·å¾ææä¹çç»æã è¯·åèç« è
    :ref:`preprocessing` ï¼é£éä¼æä¾å°æ´å¤å³äºç¼©æ¾åè§æ´ã

  * å¨ :class:`NuSVC`/:class:`OneClassSVM`/:class:`NuSVR` åçåæ° ``nu`` ï¼
    è¿ä¼¼æ¯è®­ç»è¯¯å·®åæ¯æåéçæ¯å¼ã

  * å¨ :class:`SVC`, ï¼å¦æåç±»å¨çæ°æ®ä¸åè¡¡ï¼å°±æ¯è¯´ï¼å¾å¤æ­£ä¾å¾å°è´ä¾ï¼ï¼è®¾ç½® ``class_weight='balanced'`` 
    ä¸/æå°è¯ä¸åçæ©ç½ç³»æ° ``C`` ã	

  * å¨æåæ¨¡åæ¶ï¼åºå± :class:`LinearSVC` æä½ä½¿ç¨äºéæºæ°çæå¨å»éæ©ç¹å¾ã
    æä»¥ä¸è¦æå°æå¤ï¼å¯¹äºç¸åçæ°æ®è¾å¥ï¼ä¹ä¼ç¥æä¸åçè¾åºç»æãå¦æè¿ä¸ªåçäºï¼
    å°è¯ç¨æ´å°çtol åæ°ã

  * ä½¿ç¨ç± ``LinearSVC(loss='l2', penalty='l1',
    dual=False)`` æä¾çL1æ©ç½å»äº§çç¨çè§£ï¼ä¹å°±æ¯è¯´ï¼ç¹å¾æéçå­éä¸åäºé¶ï¼è¿æ ·åæå©äºå³ç­å½æ°ã
    éçå¢å  ``C`` ä¼äº§çä¸ä¸ªæ´å¤æçæ¨¡åï¼è¦åæ´å¤çç¹å¾éæ©ï¼ãå¯ä»¥ä½¿ç¨ :func:`l1_min_c` å»è®¡ç® ``C`` çæ°å¼ï¼å»äº§çä¸ä¸ª"null" æ¨¡åï¼ææçæéç­äºé¶ï¼ã


.. _svm_kernels:

åæ ¸å½æ°
================

*åæ ¸å½æ°* å¯ä»¥æ¯ä»¥ä¸ä»»ä½å½¢å¼ï¼:

  * çº¿æ§: :math:`\langle x, x'\rangle`.

  * å¤é¡¹å¼: :math:`(\gamma \langle x, x'\rangle + r)^d`.
    :math:`d` æ¯å³é®è¯ ``degree``, :math:`r` æå® ``coef0``ã

  * rbf: :math:`\exp(-\gamma \|x-x'\|^2)`. :math:`\gamma` æ¯å³é®è¯ ``gamma``, å¿é¡»å¤§äº0ã

  * sigmoid (:math:`\tanh(\gamma \langle x,x'\rangle + r)`),
    å¶ä¸­ :math:`r` æå® ``coef0``ã

åå§åæ¶ï¼ä¸ååæ ¸ç±ä¸åçå½æ°åè°ç¨::

    >>> linear_svc = svm.SVC(kernel='linear')
    >>> linear_svc.kernel
    'linear'
    >>> rbf_svc = svm.SVC(kernel='rbf')
    >>> rbf_svc.kernel
    'rbf'


èªå®ä¹åæ ¸
--------------

æ¨å¯ä»¥èªå®ä¹èªå·±çåæ ¸ï¼éè¿ä½¿ç¨pythonå½æ°ä½ä¸ºåæ ¸æèéè¿é¢è®¡ç®Gramç©éµã

èªå®ä¹åæ ¸çåç±»å¨åå«çåç±»å¨ä¸æ ·ï¼é¤äºä¸é¢è¿å ç¹:

    * ç©ºé´ ``support_vectors_`` ç°å¨ä¸æ¯ç©ºç, åªææ¯æåéçç´¢å¼è¢«å­å¨å¨ ``support_``

    * è¯·æ ``fit()`` æ¨¡åä¸­çç¬¬ä¸ä¸ªåæ°çå¼ç¨ï¼ä¸æ¯å¯æ¬ï¼å­å¨ä¸ºå°æ¥çå¼ç¨ã
      å¦æå¨ ``fit()`` å ``predict()`` ä¹é´ææ°ç»åçæ¹åï¼æ¨å°ä¼ç¢°å°ææå¤çç»æã


ä½¿ç¨pythonå½æ°ä½ä¸ºåæ ¸
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

å¨æé æ¶ï¼æ¨åæ ·å¯ä»¥éè¿ä¸ä¸ªå½æ°ä¼ éå°å³é®è¯ ``kernel`` ï¼æ¥ä½¿ç¨æ¨èªå·±å®ä¹çåæ ¸ã

æ¨çåæ ¸å¿é¡»è¦ä»¥ä¸¤ä¸ªç©éµä½ä¸ºåæ°ï¼å¤§å°åå«æ¯
``(n_samples_1, n_features)``, ``(n_samples_2, n_features)``
åè¿åä¸ä¸ªåæ ¸ç©éµï¼å¤§å°æ¯ ``(n_samples_1, n_samples_2)``.

ä»¥ä¸ä»£ç å®ä¹ä¸ä¸ªçº¿æ§æ ¸ï¼åæé ä¸ä¸ªä½¿ç¨è¯¥åæ ¸çåç±»å¨ä¾å­::

    >>> import numpy as np
    >>> from sklearn import svm
    >>> def my_kernel(X, Y):
    ...     return np.dot(X, Y.T)
    ...
    >>> clf = svm.SVC(kernel=my_kernel)

.. topic:: ä¾å­:

 * :ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`.

ä½¿ç¨Gramç©éµ
~~~~~~~~~~~~~~~~~~~~~

å¨éåºç®æ³ä¸­ï¼è®¾ç½® ``kernel='precomputed'`` åæXæ¿æ¢ä¸ºGramç©éµã
æ­¤æ¶ï¼å¿é¡»è¦æä¾å¨ *ææ* è®­ç»ç¢éåæµè¯ç¢éä¸­çåæ ¸å¼ã 

    >>> import numpy as np
    >>> from sklearn import svm
    >>> X = np.array([[0, 0], [1, 1]])
    >>> y = [0, 1]
    >>> clf = svm.SVC(kernel='precomputed')
    >>> # çº¿æ§åæ ¸è®¡ç®
    >>> gram = np.dot(X, X.T)
    >>> clf.fit(gram, y) # doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='auto',
        kernel='precomputed', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> # é¢æµè®­ç»æ ·æ¬
    >>> clf.predict(gram)
    array([0, 1])

RBFåæ ¸åæ°
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

å½ç¨ *å¾ååº* (RBF)åæ ¸å»è®­ç»SVMï¼æä¸¤ä¸ªåæ°å¿é¡»è¦å»èèï¼ ``C`` æ©ç½ç³»æ°å ``gamma`` ãåæ° ``C`` ï¼
éç¨å¨ææSVMåæ ¸ï¼ä¸å³ç­è¡¨é¢çç®åæ§ç¸æè¡¡ï¼å¯ä»¥å¯¹è®­ç»æ ·æ¬çè¯¯åç±»è¿è¡æä»·è½¬æ¢ã
è¾å°ç ``C`` ä¼ä½¿å³ç­è¡¨é¢æ´å¹³æ»ï¼åæ¶è¾é«ç ``C`` æ¨å¨æ­£ç¡®å°åç±»ææè®­ç»æ ·æ¬ã ``Gamma`` å®ä¹äºåä¸
è®­ç»æ ·æ¬è½èµ·å°å¤å¤§çå½±åãè¾å¤§ç ``gamma`` ä¼æ´è®©å¶ä»æ ·æ¬åå°å½±åã

éæ©åéç ``C`` å ``gamma`` ï¼å¯¹SVMçæ§è½èµ·å°å¾å³é®çä½ç¨ãå»ºè®®ä¸ç¹æ¯
ä½¿ç¨ Â :class:`sklearn.model_selection.GridSearchCV` ä¸ ``C`` å ``gamma`` ç¸é
æåå·®è·ä»èéæ©å°å¥½çæ°å¼ã

.. topic:: ä¾å­:

 * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`

.. _svm_mathematical_formulation:

æ°å­¦å¬å¼
========================

æ¯æåéæºå¨é«ç»´åº¦ææ ç©·ç»´åº¦ç©ºé´ä¸­ï¼æå»ºä¸ä¸ªè¶å¹³é¢æèä¸ç³»åçè¶å¹³é¢ï¼å¯ä»¥ç¨äºåç±»ãåå½æèå«çä»»å¡ã
ç´è§å°çï¼åå©è¶å¹³é¢å»å®ç°ä¸ä¸ªå¥½çåå²ï¼ è½å¨ä»»æç±»å«ä¸­ä½¿æä¸ºæ¥è¿çè®­ç»æ°æ®ç¹å·ææå¤§çé´éè·ç¦»ï¼å³æ
è°çå½æ°ä½éï¼ï¼è¿æ ·åæ¯å ä¸ºéå¸¸æ´å¤§çä½éè½ææ´ä½çåç±»å¨æ³åè¯¯å·®ã


.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_001.png
   :align: center
   :scale: 75

SVC
---

å¨ä¸¤ç±»ä¸­ï¼ç»å®è®­ç»åé :math:`x_i \in \mathbb{R}^p`, i=1,..., n, åä¸ä¸ªåé :math:`y \in \{1, -1\}^n`, SVCè½è§£å³
å¦ä¸ä¸»è¦é®é¢:

.. math::

    \min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i



    \textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
    & \zeta_i \geq 0, i=1, ..., n

å®çå¯¹å¶æ¯

.. math::

   \min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha


   \textrm {subject to } & y^T \alpha = 0\\
   & 0 \leq \alpha_i \leq C, i=1, ..., n

å¶ä¸­ :math:`e` æ¯ææçåéï¼ :math:`C > 0` æ¯ä¸çï¼:math:`Q` æ¯ä¸ä¸ª :math:`n` ç± :math:`n` ä¸ªåæ­£å®ç©éµï¼
è :math:`Q_{ij} \equiv y_i y_j K(x_i, x_j)` ï¼å¶ä¸­ :math:`K(x_i, x_j) = \phi (x_i)^T \phi (x_j)` æ¯åæ ¸ãæä»¥è®­ç»åéæ¯éè¿å½æ° :math:`\phi`ï¼é´æ¥åæ å°ä¸ä¸ªæ´é«ç»´åº¦çï¼æ ç©·çï¼ç©ºé´ã


å³ç­å½æ°æ¯:

.. math:: \operatorname{sgn}(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)

æ³¨æ::

è½ç¶è¿äºSVMæ¨¡åæ¯ä» `libsvm`_ å `liblinear`_ ä¸­æ´¾çåºæ¥ï¼ä½¿ç¨äº ``C`` ä½ä¸ºè°æ´åæ°ï¼ä½æ¯å¤§å¤æ°ç
æ»å»ä½¿ç¨äº ``alpha``ãä¸¤ä¸ªæ¨¡åçæ­£ååéä¹é´çç²¾ç¡®ç­ä»·ï¼åå³äºæ¨¡åä¼åçåç¡®ç®æ å½æ°ãä¸¾
ä¸ªä¾å­ï¼å½ä½¿ç¨çä¼°è®¡å¨æ¯ :class:`sklearn.linear_model.Ridge <ridge>` ååå½æ¶ï¼ä»ä»¬ä¹é´çç¸å³æ§æ¯ :math:`C = \frac{1}{alpha}`ã 



è¿äºåæ°è½éè¿æå ``dual_coef_``ã ``support_vectors_`` ã ``intercept_`` å»è®¿é®ï¼è¿äºæååå«æ§å¶äºè¾åº :math:`y_i \alpha_i`ãæ¯æåéåæ å³é¡¹ :math:`\rho` ï¼ 

.. topic:: åèæç®:

 * `"Automatic Capacity Tuning of Very Large VC-dimension Classifiers"
   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215>`_,
   I. Guyon, B. Boser, V. Vapnik - Advances in neural information
   processing 1993.


 * `"Support-vector networks"
   <http://link.springer.com/article/10.1007%2FBF00994018>`_,
   C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).



NuSVC
-----

æä»¬å¼å¥ä¸ä¸ªæ°çåæ° :math:`\nu` æ¥æ§å¶æ¯æåéçæ°éåè®­ç»è¯¯å·®ãåæ° :math:`\nu \in (0,
1]` æ¯è®­ç»è¯¯å·®åæ°çä¸éåæ¯æåéåæ°çä¸éã

å¯ä»¥çåºï¼ :math:`\nu`-SVC å¬å¼æ¯ :math:`C`-SVC çååæ°åï¼æä»¥æ°å­¦ä¸æ¯ç­æçã


SVR
---

ç»å®è®­ç»åé :math:`x_i \in \mathbb{R}^p`, i=1,..., nï¼åé :math:`y \in \mathbb{R}^n` :math:`\varepsilon`-SVR 
è½è§£å³ä»¥ä¸çä¸»è¦é®é¢ï¼


.. math::

    \min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)



    \textrm {subject to } & y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\
                          & w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\
                          & \zeta_i, \zeta_i^* \geq 0, i=1, ..., n

å®çå¯¹å¶æ¯

.. math::

   \min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)


   \textrm {subject to } & e^T (\alpha - \alpha^*) = 0\\
   & 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n


å¶ä¸­ :math:`e` æ¯ææçåéï¼ :math:`C > 0` æ¯ä¸çï¼:math:`Q` æ¯ä¸ä¸ª :math:`n` ç± :math:`n` ä¸ªåæ­£å®ç©éµï¼
è :math:`Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)` æ¯åæ ¸ã
æä»¥è®­ç»åéæ¯éè¿å½æ° :math:`\phi`ï¼é´æ¥åæ å°ä¸ä¸ªæ´é«ç»´åº¦çï¼æ ç©·çï¼ç©ºé´ã


å³ç­å½æ°æ¯:

.. math:: \sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + \rho

è¿äºåæ°è½éè¿æå ``dual_coef_``ã ``support_vectors_`` ã ``intercept_`` å»è®¿é®ï¼è¿äº
æååå«æ§å¶äºä¸åç :math:`\alpha_i - \alpha_i^*`ãæ¯æåéåæ å³é¡¹ :math:`\rho`ï¼


.. topic:: åèæç®:

 * `"A Tutorial on Support Vector Regression"
   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288>`_,
   Alex J. Smola, Bernhard SchÃ¶lkopf - Statistics and Computing archive
   Volume 14 Issue 3, August 2004, p. 199-222. 


.. _svm_implementation_details:

å®ç°ç»è
======================

å¨åºå±éï¼æä»¬ä½¿ç¨ `libsvm`_ å `liblinear`_ å»å¤çææçè®¡ç®ãè¿äºåºé½ä½¿ç¨äº C å Cython å»åè£ã


.. _`libsvm`: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
.. _`liblinear`: http://www.csie.ntu.edu.tw/~cjlin/liblinear/

.. topic:: åèæç®:

  æå³å®ç°çæè¿°åä½¿ç¨ç®æ³çç»èï¼è¯·åè

    - `LIBSVM: A Library for Support Vector Machines
      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_.

    - `LIBLINEAR -- A Library for Large Linear Classification
      <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`_.


